{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of E-Commerce Clothing Reviews\n",
    "\n",
    "Training an evaluation set from <a href=\"https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews\">kaggle</a>\n",
    "\n",
    "Approach\n",
    "* set each review rated higher than or equal to 3 as positive review\n",
    "* set each review rated below 3 as negative review\n",
    "* build embedding matrix using Google's pre-trained word2vec model\n",
    "* train CNN classifier for binary sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marku\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# word embeddings\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "\n",
    "# CNN architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Activation, Dropout, BatchNormalization\n",
    "\n",
    "# CNN training\n",
    "from keras import callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# CNN load models\n",
    "from keras.models import load_model\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "* load data\n",
    "* select columns (title, review text and rating)\n",
    "* drop all rows that which don't have at least a rating and review text\n",
    "* combine title and review text to one string separated by a punct\n",
    "* remove non-word/-number/-punctuation characters\n",
    "* remove english stopwords\n",
    "* remove the five most common words that appear in both classes\n",
    "* label each positive rating with 0\n",
    "* label each negative rating with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nan . absolutely wonderful silky sexy comfortable', 'nan . love ! sooo pretty . happened find store glad bc never would ordered online bc petite . bought petite 5 8 . love length hits little knee . would definitely true midi someone truly petite .', 'major design flaws . high hopes really wanted work . initially ordered petite small usual found outrageously small . small fact could zip ! reordered petite medium ok. overall half comfortable nicely bottom half tight layer several somewhat cheap net layers . imo major design flaw net layer sewn directly zipper c', 'favorite buy ! . love love love jumpsuit . fun flirty fabulous ! every time wear get nothing great compliments !', 'flattering shirt . shirt flattering due adjustable front tie . perfect length wear leggings sleeveless pairs well cardigan . love shirt ! ! !', 'petite . love tracy reese dresses one petite . 5 feet tall usually wear 0p brand . pretty package lot . skirt long full overwhelmed small frame . stranger alterations shortening narrowing skirt would take away embellishment garment . love color idea style work . returned .', 'cagrcoal shimmer fun . aded basket hte last mintue see would look person . store pick . went teh darkler color pale hte color really gorgeous turns mathced everythiing trying prefectly . little baggy hte xs hte msallet bummer petite . decided jkeep though said matvehd everything . ejans pants 3 skirts waas trying kept oops .', 'shimmer surprisingly goes lots . ordered carbon store pick ton stuff always try used pair skirts pants . everything went . color really nice charcoal shimmer went well pencil skirts flare pants etc . compaint bit big sleeves long go petite . also bit loose xxs kept wil ldecide later since light color already sold hte smallest', 'flattering . love . usually get xs runs little snug bust ordered . flattering feminine usual retailer flair style .', 'fun ! . 5 5 125 lbs . ordered petite make sure length long . typically wear xs regular retailer dresses . less busty 34b cup smaller petite perfectly snug tight . love could party work . love tulle longer fabric underneath .']\n"
     ]
    }
   ],
   "source": [
    "keep_words_and_punct = r\"[^a-zA-Z0-9?!.]|[\\.]{2,}\"\n",
    "mult_whitespaces = \"\\s{2,}\"\n",
    "\n",
    "df = pd.read_csv(DATA_DIR + 'review_data.csv')\n",
    "reviews = df.loc[:,('Title', 'Review Text', 'Rating')]\n",
    "reviews.dropna(how=\"any\", inplace=True, subset=['Review Text', 'Rating'])\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "duplicate_words = ['dress', 'size', 'top', 'fit', 'like']\n",
    "\n",
    "for i, row in reviews.iterrows():\n",
    "    review = str(row['Title']) + '. ' + str(row['Review Text'])\n",
    "    clean_review = re.sub(mult_whitespaces, ' ', re.sub(keep_words_and_punct, ' ', str(review).lower()))\n",
    "    tokens = word_tokenize(clean_review)\n",
    "    filtered_sentence = [word for word in tokens if not word in stop_words and not word in duplicate_words]\n",
    "    sentences = \" \".join(filtered_sentence)\n",
    "\n",
    "    if row['Rating'] >= 3:\n",
    "        texts.append(sentences)\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        texts.append(sentences)\n",
    "        labels.append(1)\n",
    "\n",
    "print(texts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize reviews and find longest word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14602 unique tokens.\n",
      "max sequence len: 67\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "max_sequence_len = 0\n",
    "for sequence in sequences:\n",
    "    if len(sequence) > max_sequence_len:\n",
    "        max_sequence_len = len(sequence)\n",
    "print(\"max sequence len: %i\" % max_sequence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad all sequences to the longest length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (22641, 67)\n",
      "Shape of label tensor: (22641, 2)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=max_sequence_len)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into a training set and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec model from Google - trained on google news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(DATA_DIR + 'word embeddings/GoogleNews-vectors-negative300.bin', binary=True, limit=25000)\n",
    "\n",
    "embeddings_index = {}\n",
    "for word in range(len(model.vocab)):\n",
    "    embedding_vector = model[model.index2word[word]]\n",
    "    embeddings_index[model.index2word[word]] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe model from Stanford University - trained on 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE PRETRAINED GLOVE WORD EMBEDDINGS (trained on 20 newsgroups)\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "if os.path.isfile(DATA_DIR + 'word embeddings/glove_embeddings_index.pkl'):\n",
    "    with open(DATA_DIR + 'word embeddings/glove_embeddings_index.pkl', 'rb') as file:\n",
    "        embeddings_index = pickle.load(file)\n",
    "else:\n",
    "    embeddings_index = {}\n",
    "    f = open(DATA_DIR + 'word embeddings/glove.6B.100d.txt', encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    with open(DATA_DIR + 'word embeddings/glove_embeddings_index.pkl', 'wb') as file:\n",
    "        pickle.dump(embeddings_index, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Convolutional Neural Network (CNN)\n",
    "### set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "FILTERS = 300\n",
    "KERNEL_SIZE = 3\n",
    "HIDDEN_DIMS = 250\n",
    "EPOCHS = 50\n",
    "P_DROPOUT = 0.2\n",
    "labels_index = {'pos': 0, 'neg': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the structure/architecture\n",
    "Embedding layer > convolution layer > sigmoid function for classification\n",
    "\n",
    "In between dropout and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_sequence_len,\n",
    "                    trainable=False))  # prevent keras from updating the word indices during training process\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(FILTERS,\n",
    "                 KERNEL_SIZE,\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(HIDDEN_DIMS))\n",
    "model.add(Dropout(P_DROPOUT))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(len(labels_index)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18113 samples, validate on 4528 samples\n",
      "Epoch 1/50\n",
      " - 32s - loss: 0.2605 - acc: 0.9011 - val_loss: 0.1969 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.19686, saving model to ../Data/sentiment_sequential.hdf5\n",
      "Epoch 2/50\n",
      " - 39s - loss: 0.1843 - acc: 0.9203 - val_loss: 0.1912 - val_acc: 0.9153\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.19686 to 0.19124, saving model to ../Data/sentiment_sequential.hdf5\n",
      "Epoch 3/50\n",
      " - 35s - loss: 0.1354 - acc: 0.9438 - val_loss: 0.2174 - val_acc: 0.9142\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.19124\n",
      "Epoch 4/50\n",
      " - 35s - loss: 0.0938 - acc: 0.9626 - val_loss: 0.2455 - val_acc: 0.9142\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.19124\n",
      "Epoch 5/50\n",
      " - 33s - loss: 0.0624 - acc: 0.9761 - val_loss: 0.2836 - val_acc: 0.8954\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.19124\n",
      "Epoch 6/50\n",
      " - 37s - loss: 0.0181 - acc: 0.9942 - val_loss: 0.3600 - val_acc: 0.9125\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.19124\n",
      "Epoch 7/50\n",
      " - 36s - loss: 0.0071 - acc: 0.9990 - val_loss: 0.3968 - val_acc: 0.9121\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.19124\n",
      "Epoch 8/50\n",
      " - 37s - loss: 0.0043 - acc: 0.9997 - val_loss: 0.4172 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.19124\n",
      "Epoch 9/50\n",
      " - 35s - loss: 0.0026 - acc: 0.9998 - val_loss: 0.4180 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.19124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23c60aa5c18>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=DATA_DIR + 'sentiment_sequential.hdf5', verbose=1, save_best_only=True)\n",
    "earlyStopper = EarlyStopping(monitor='val_loss', min_delta=0, patience=7, verbose=0, mode='auto')\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=BATCH_SIZE,<\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=(x_val, y_val),\n",
    "          callbacks=[checkpointer, earlyStopper, reduce_lr],\n",
    "          verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the stored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 91.53%\n",
      "Loss: 19.12%\n"
     ]
    }
   ],
   "source": [
    "best_model = load_model(DATA_DIR + 'sentiment_sequential.hdf5')\n",
    "\n",
    "scores = best_model.evaluate(x_val, y_val, verbose=0, batch_size=BATCH_SIZE)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Loss: %.2f%%\" % (scores[0]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introduction-to-ml",
   "language": "python",
   "name": "introduction-to-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
