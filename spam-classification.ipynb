{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is an introduction to Machine Learning with scikit-learn\n",
    "### We will build a simple spam-filter trained on some example E-Mails\n",
    "\n",
    "First let's import all the necessary libraries <br/>\n",
    "We will use pandas as well ;-)\n",
    "\n",
    "<span style=\"color:red\">Note: If some of the imports won't work, make sure you have them installed using pip or anaconda</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Next we will read the spam and no spam (i. e. ham) examples from the data directory. <br/>\n",
    "Make sure you have the examples in the <i>data</i> directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE = '\\n'\n",
    "\n",
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\n",
    "SOURCES = [\n",
    "    ('data/ham/beck-s',      HAM),\n",
    "    ('data/ham/farmer-d',    HAM),\n",
    "    ('data/ham/kaminski-v',  HAM),\n",
    "    ('data/ham/kitchen-l',   HAM),\n",
    "    ('data/ham/lokay-m',     HAM),\n",
    "    ('data/ham/williams-w3', HAM),\n",
    "    ('data/spam/BG',          SPAM),\n",
    "    ('data/spam/GP',          SPAM),\n",
    "    ('data/spam/SH',          SPAM)\n",
    "]\n",
    "\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "''' iterate through all files an yield the email body '''\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    past_header, lines = False, []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        if past_header:\n",
    "                            lines.append(line)\n",
    "                        elif line == NEWLINE:\n",
    "                            past_header = True\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a pandas dataframe for the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will concate the dataframes using pandas' append method. <br/>\n",
    "<span style=\"color:red\">Note: This may take some time. Keep calm and let it finish ;-) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification), sort=True)\n",
    "\n",
    "data = data.reindex(numpy.random.permutation(data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n",
    "We start with the basic CountVectorizer, i. e. a bag-of-words approach <br/>\n",
    "<span style=\"color:red\">Note: This may take some time. Keep calm and let it finish ;-) </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "counts = count_vectorizer.fit_transform(data['text'].values)\n",
    "labels = data['class'].values\n",
    "\n",
    "# split data into test and training set - hold 20% out for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(counts, labels, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Naive Bayes Classifier\n",
    "Train the classifier with the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = MultinomialNB()\n",
    "targets = data['class'].values\n",
    "nb_clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.992\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.3f' % nb_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test some other scores like precision, recall and f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.954\n",
      "Recall: 0.960\n",
      "F1-Score: 0.955\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred = nb_clf.predict(X_test)\n",
    "# we are using macro to evalutate the overall performance of the classifier and averaging the weights of all classes equally\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))\n",
    "print('F1-Score: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now try what happens when you are changing the score to use 'micro' averaging\n",
    "How do these results differ from the previous results and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a8d5d3975e7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO fill in the averaging mode 'micro'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Precision: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Recall: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Recall: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO fill in the averaging mode 'micro'\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average=\" \"))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average=\" \"))\n",
    "print('Recall: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average=\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate prediction with an example\n",
    "Now that we have seen the score, let's test it on an example. <br/>\n",
    "Is the prediction correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spam'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = [\"Free Viagra call today\"]\n",
    "example_counts = count_vectorizer.transform(example)\n",
    "prediction = nb_clf.predict(example_counts);\n",
    "prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more examples. Are the predictions also correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-Mail 0: spam\n",
      "E-Mail 1: ham\n",
      "E-Mail 2: spam\n",
      "E-Mail 3: ham\n"
     ]
    }
   ],
   "source": [
    "examples = [\"Free Viagra call today!\", \n",
    "            \"I'm going to attend the Python Learning group tomorrow.\",\n",
    "            \"Free Viagra Free Viagra Free Viagra\",\n",
    "            \"Today we will learn about Machine Learning\"]\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "predictions = nb_clf.predict(example_counts)\n",
    "for i in range(len(predictions)):\n",
    "    print(\"E-Mail %i: %s\"%(i, predictions[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try your own example. Type in some E-Mail text inside the quotes and see if the prediction is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spam']\n"
     ]
    }
   ],
   "source": [
    "your_test_mail = [\" ... \"]\n",
    "test_mail_counts = count_vectorizer.transform(your_test_mail)\n",
    "print(nb_clf.predict(test_mail_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with Support Vector Machine\n",
    "Tasks:<br/>\n",
    "1. Instantiate the classifier with default Parameters (empty brackets)\n",
    "2. Train the classifier with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_clf =  #TODO\n",
    "svm_clf.fit(X_, y_) #TODO texts, labels from training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's how good the accuracy score is compared to the Naive Bayes Classifier <br/>\n",
    "Is it better or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy: %.3f' % svm_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to look at the other scores as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_clf.predict(X_test)\n",
    "# we are using macro to evalutate the overall performance of the classifier and averaging the weights of all classes equally\n",
    "print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))\n",
    "print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))\n",
    "print('F1-Score: %.3f' % f1_score(y_true=y_test, y_pred=y_pred, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And test it with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = [\"Free Viagra call today\"]\n",
    "example_counts = count_vectorizer.transform(example)\n",
    "prediction = svm_clf.predict(example_counts);\n",
    "prediction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with K-Nearest-Neighbour\n",
    "Now it's your turn to program a classification using scikit-learn's K-Nearest-Neighbour implementation.<br/>\n",
    "Validate the classifier using the same apporoaches as above. <br/>\n",
    "How does K-Nearest-Neighbour perfom on this data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = #TODO\n",
    "knn_clf.fit(#TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we use TfidfVectorizer instead of CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
